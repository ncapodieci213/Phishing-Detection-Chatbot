# Load or cache ML & LLM models
# from transformers import AutoTokenizer, AutoModelForCausalLM

def load_model():
	# tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
	# model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
	# messages = [
	# 	{"role": "user", "content": "Who are you?"},
	# ]
	# inputs = tokenizer.apply_chat_template(
	# 	messages,
	# 	add_generation_prompt=True,
	# 	tokenize=True,
	# 	return_dict=True,
	# 	return_tensors="pt",
	# ).to(model.device)

	# outputs = model.generate(**inputs, max_new_tokens=40)
	# print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))

	tokenizer = 0
	model = 1
	return tokenizer, model